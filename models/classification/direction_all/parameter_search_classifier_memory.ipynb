{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras.preprocessing import image_dataset_from_directory\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.utils import Sequence\n",
    "from keras.models import load_model\n",
    "from tensorflow.distribute import MirroredStrategy\n",
    "\n",
    "#initialise random generator\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define FelixDataflow classes and functions.\n",
    "\n",
    "class FelixSequence(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size, file_type):\n",
    "        \"\"\"Here self.x is a list of paths to file_type files. self.y is a\n",
    "        corresponding list of labels.\"\"\"\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.file_type = file_type\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return arrs_from_paths(batch_x, self.file_type), to_categorical(np.array(batch_y),10)\n",
    "\n",
    "def gen_paths_labels(base_path):\n",
    "    \"\"\"A generator to yield (data-paths, corresponding labels) tuples for each\n",
    "    segment of data (typically training, validation, and testing).\"\"\"\n",
    "    for segment in sorted(os.listdir(base_path)):\n",
    "        segment_path = os.path.join(base_path, segment)\n",
    "        segment_paths = []\n",
    "        segment_labels = []\n",
    "        for label in os.listdir(segment_path):\n",
    "            label_path = os.path.join(segment_path, label)\n",
    "            for crystal in os.listdir(label_path):\n",
    "                segment_paths.append(os.path.join(label_path, crystal))\n",
    "                segment_labels.append(label)\n",
    "        indexes = np.arange(len(segment_labels))\n",
    "        rng.shuffle(indexes)\n",
    "        yield [np.array(segment_paths)[indexes], np.array(list(map(int,segment_labels)))[indexes]]\n",
    "\n",
    "def arrs_from_paths(paths, file_type):\n",
    "    if file_type == \"txt\":\n",
    "        return np.array([np.loadtxt(file_name) for file_name in paths])\n",
    "    elif file_type == \"npy\":\n",
    "        return np.array([np.load(file_name)[[0],:,:] for file_name in paths])\n",
    "\n",
    "def felix_fit_new(model, batch_size, epochs, workers, AllData, file_type, patience):\n",
    "    #AllPaths = [[TrainingPaths, TrainingThickness], [], []]\n",
    "    \"\"\"A fit function to allow validation and test data to be supplied via a\n",
    "    generator.\"\"\"\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    \n",
    "    x = np.arange(0, epochs)\n",
    "    \n",
    "    TrainImage = AllData[0][0]\n",
    "    ValImage = AllData[0][1]\n",
    "    TestImage = AllData[0][2]\n",
    "    \n",
    "    TrainClassification = AllData[1][0]\n",
    "    ValClassification = AllData[1][1]\n",
    "    TestClassification = AllData[1][2]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"-------------------------------------------------------------------------\")\n",
    "        print(\"Epoch\", epoch+1, \"/\", epochs, \": \")\n",
    "        print(\"Training: \")\n",
    "        train_hist = model.fit(TrainImage, TrainClassification, validation_data = (ValImage, ValClassification), epochs = epoch+1, workers = workers, initial_epoch = epoch, shuffle=True)\n",
    "\n",
    "        print(\"Validation: \")\n",
    "     \n",
    "        epoch_loss = train_hist.history[\"val_loss\"][-1]\n",
    "        if(epoch_loss < best_val_loss):\n",
    "            #model.save(NewPath+ModelName)\n",
    "            print(\"The model improved from: \",best_val_loss, \"to: \", epoch_loss)\n",
    "            best_val_loss = epoch_loss\n",
    "            patience_i = 0\n",
    "        else:\n",
    "            patience_i+=1\n",
    "            print(\"The model did not improve, patience_i = \", patience_i)\n",
    "\n",
    "        print(\"Epoch loss: \", epoch_loss)\n",
    "        #val_hist[0][epoch] = avg_recon_loss\n",
    "        if(patience_i > patience):\n",
    "            print(\"Early Stopping, the model did not improve from: \", best_val_loss)\n",
    "            break\n",
    "\n",
    "    print(\"-------------------------------------------------------------------------\")\n",
    "    print(\"Testing: \")\n",
    "    tst_hist = model.evaluate(TestImage, TestClassification, workers = workers)\n",
    "    \n",
    "    return tst_hist[0]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeThicknessList(ListPaths):\n",
    "    Thickness = []\n",
    "    for i in ListPaths:\n",
    "        Thickness.append(int(i.split(\"/\")[-1].split(\".\")[0]))\n",
    "    Thickness = np.array(Thickness)\n",
    "    return(Thickness)\n",
    "\n",
    "def OpenTxt(Path):\n",
    "    with open(Path) as textFile:\n",
    "        lines = [line.split() for line in textFile]\n",
    "    List = []\n",
    "    for i in lines:\n",
    "        List.append(i[0])\n",
    "    return(List)\n",
    "\n",
    "def LoadCentralBeam(AllPaths):\n",
    "    TrainImages = np.zeros(128 * 128 * len(AllPaths[0]), dtype = np.float).reshape(len(AllPaths[0]), 1, 128, 128)\n",
    "    ValImages = np.zeros(128 * 128 * len(AllPaths[1]), dtype = np.float).reshape(len(AllPaths[1]), 1, 128, 128)\n",
    "    TestImages = np.zeros(128 * 128 * len(AllPaths[2]), dtype = np.float).reshape(len(AllPaths[2]), 1, 128, 128)\n",
    "    \n",
    "    AllImages = [TrainImages, ValImages, TestImages]\n",
    "    \n",
    "    for i in range(0, len(AllImages)):\n",
    "        for j in range(0, len(AllImages[i])):\n",
    "            #print(j)\n",
    "            AllImages[i][j] = np.load(AllPaths[i][j])[0]\n",
    "    return AllImages\n",
    "\n",
    "DataPath = \"//home/ug-ml/felix-ML/classification/Classification000/DataPaths/\"\n",
    "\n",
    "TrainPath = OpenTxt(DataPath + \"Train_0p1.txt\")\n",
    "ValPath = OpenTxt(DataPath + \"Validation_0p1.txt\")\n",
    "TestPath = OpenTxt(DataPath + \"Test_0p1.txt\")\n",
    "\n",
    "AllLACBED = LoadCentralBeam([TrainPath, ValPath, TestPath])\n",
    "\n",
    "\n",
    "TrainThickness = to_categorical(MakeThicknessList(TrainPath),10)\n",
    "ValThickness = to_categorical(MakeThicknessList(ValPath),10)\n",
    "TestThickness = to_categorical(MakeThicknessList(TestPath),10)\n",
    "\n",
    "AllData = [AllLACBED, [TrainThickness, ValThickness, TestThickness]] #[Lacbed image, thickness]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All paths\n",
    "\n",
    "SaveDataPath = \"/home/ug-ml/Documents/GitHub_BigFiles/SaveFolder\" #Base directory of place you store information of models\n",
    "SaveFolderName = \"/Classifer_1\" #Will create a folder and put in information about the outcome / inputs\n",
    "ModelName = \"/Model.hdf5\"\n",
    "\n",
    "\n",
    "#Many variables\n",
    "\n",
    "#Model Variables\n",
    "input_shape = (1, 128, 128)\n",
    "\n",
    "#Hyper parameters\n",
    "learning_rate = 0.0005\n",
    "l2_regularizer = 0.0001\n",
    "loss = 'categorical_crossentropy'\n",
    "optimizer = \"RMSprop\" #Not a variable ONLY used for a note\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "ShuffleTrainData = True\n",
    "\n",
    "#Call back variables\n",
    "TrainingPatience = 5\n",
    "CheckPointMonitor = 'val_acc'\n",
    "EarlyStopMonitor = 'val_acc'\n",
    "\n",
    "#CPU variables\n",
    "CPUworkers = 16\n",
    "\n",
    "\n",
    "#List the name of the variables you want to save in a file\n",
    "VariableListName = [\"input_shape\", \n",
    "                   \"learning_rate\", \"l2_regularizer\", \"loss\", \"optimizer\", \"batch_size\", \"epochs\", \"ShuffleTrainData\",\n",
    "                   \"TrainingPatience\", \"CheckPointMonitor\", \"EarlyStopMonitor\",\n",
    "                   \"CPUworkers\"]\n",
    "\n",
    "#List the variables in the same order as VariableListName\n",
    "VariableListValues = [input_shape, \n",
    "                   learning_rate, l2_regularizer, loss, optimizer, batch_size, epochs, ShuffleTrainData,\n",
    "                   TrainingPatience, CheckPointMonitor, EarlyStopMonitor,\n",
    "                   CPUworkers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(learning_rate, l2_regularizer):\n",
    "    strategy = MirroredStrategy() #Allows multiple GPUs\n",
    "\n",
    "    with strategy.scope():\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Conv2D(128, (4, 4),\n",
    "                                         activation='relu',\n",
    "                                         data_format='channels_first',\n",
    "                                         input_shape= input_shape))\n",
    "        model.add(layers.MaxPooling2D((2, 2), data_format='channels_first'))\n",
    "        model.add(layers.Conv2D(128, (4, 4),\n",
    "                                         data_format='channels_first',\n",
    "                                         activation='relu'))\n",
    "        model.add(layers.MaxPooling2D((2, 2), data_format='channels_first'))\n",
    "        model.add(layers.Conv2D(128, (4, 4),\n",
    "                                         data_format='channels_first',\n",
    "                                         activation='relu'))\n",
    "        model.add(layers.MaxPooling2D((2, 2), data_format='channels_first'))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dropout(0.25))\n",
    "        model.add(layers.Dense(128, activation='relu',\n",
    "                               kernel_regularizer = l2(l2_regularizer)))\n",
    "\n",
    "        model.add(layers.Dense(10, activation='softmax',\n",
    "                               kernel_regularizer = l2(l2_regularizer)))\n",
    "\n",
    "        model.compile(loss = loss,\n",
    "                      optimizer = optimizers.RMSprop(learning_rate = learning_rate),\n",
    "                      metrics=['acc'])\n",
    "        \n",
    "        \n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 128, 125, 125)     2176      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 128, 62, 62)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 59, 59)       262272    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 128, 29, 29)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 128, 26, 26)       262272    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 128, 13, 13)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 21632)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 21632)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               2769024   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 3,297,034\n",
      "Trainable params: 3,297,034\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 1 / 50 : \n",
      "Training: \n",
      "WARNING:tensorflow:From /home/ug-ml/felix-ML/env/lib/python3.6/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n",
      "INFO:tensorflow:batch_all_reduce: 10 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 10 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "1769/1769 [==============================] - ETA: 0s - loss: 1.6911 - acc: 0.3533INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "1769/1769 [==============================] - 88s 50ms/step - loss: 1.6911 - acc: 0.3533 - val_loss: 1.6722 - val_acc: 0.3748\n",
      "Validation: \n",
      "The model improved from:  inf to:  1.6721724271774292\n",
      "Epoch loss:  1.6721724271774292\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 2 / 50 : \n",
      "Training: \n",
      "Epoch 2/2\n",
      "1769/1769 [==============================] - 82s 46ms/step - loss: 1.3416 - acc: 0.5253 - val_loss: 1.3121 - val_acc: 0.5474\n",
      "Validation: \n",
      "The model improved from:  1.6721724271774292 to:  1.312055230140686\n",
      "Epoch loss:  1.312055230140686\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 3 / 50 : \n",
      "Training: \n",
      "Epoch 3/3\n",
      "1769/1769 [==============================] - 82s 47ms/step - loss: 1.1733 - acc: 0.6077 - val_loss: 1.2398 - val_acc: 0.5892\n",
      "Validation: \n",
      "The model improved from:  1.312055230140686 to:  1.2397675514221191\n",
      "Epoch loss:  1.2397675514221191\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 4 / 50 : \n",
      "Training: \n",
      "Epoch 4/4\n",
      "1769/1769 [==============================] - 81s 46ms/step - loss: 1.0765 - acc: 0.6568 - val_loss: 1.1732 - val_acc: 0.6145\n",
      "Validation: \n",
      "The model improved from:  1.2397675514221191 to:  1.1732031106948853\n",
      "Epoch loss:  1.1732031106948853\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 5 / 50 : \n",
      "Training: \n",
      "Epoch 5/5\n",
      "1769/1769 [==============================] - 81s 46ms/step - loss: 1.0006 - acc: 0.6918 - val_loss: 1.1186 - val_acc: 0.6665\n",
      "Validation: \n",
      "The model improved from:  1.1732031106948853 to:  1.1186270713806152\n",
      "Epoch loss:  1.1186270713806152\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 6 / 50 : \n",
      "Training: \n"
     ]
    }
   ],
   "source": [
    "prev_searched = [[0,1,2]]\n",
    "\n",
    "learning_rate = np.array([0.0005, 0.0001, 0.001, 0.002, 0.005])\n",
    "l2_regularizer = np.array([0.00005, 0.0001, 0.0005, 0.001, 0.002])\n",
    "batch_size = np.array([8, 16, 32, 64, 128])\n",
    "\n",
    "def neighbours(point):\n",
    "    dircs = np.array([[0,0,1],[0,1,0],[1,0,0],[0,0,-1],[0,-1,0],[-1,0,0]])\n",
    "    ns = dircs+point\n",
    "    return np.array([i for i in ns if (0<=i).all() and (i<5).all() and (i != prev_searched).all(axis=0).any()])\n",
    "\n",
    "z = np.array([0,1,2])\n",
    "converged = False\n",
    "best_test_loss = np.inf\n",
    "best_lr = np.nan\n",
    "best_l2_r = np.nan\n",
    "best_bs = np.nan\n",
    "\n",
    "while not converged:\n",
    "    neighs = neighbours(z)\n",
    "    #print(neighs)\n",
    "\n",
    "    lr = learning_rate[neighs[:,0]]\n",
    "    l2_r = l2_regularizer[neighs[:,1]]\n",
    "    bs = batch_size[neighs[:,2]]\n",
    "\n",
    "    step_params = np.array([lr, l2_r, bs]).T\n",
    "    \n",
    "    converged = True\n",
    "    \n",
    "    for i, param_set in enumerate(step_params):\n",
    "        prev_searched.append(neighs[i])\n",
    "        print(param_set[0])\n",
    "        \n",
    "        model = build_model(param_set[0], param_set[1])\n",
    "        \n",
    "        test_loss = felix_fit_new(model, param_set[2].astype(int), epochs, CPUworkers, AllData, \"npy\", TrainingPatience)\n",
    "        \n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            best_lr = param_set[0]\n",
    "            best_l2_r = param_set[1]\n",
    "            best_bs = param_set[2]\n",
    "            converged = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
