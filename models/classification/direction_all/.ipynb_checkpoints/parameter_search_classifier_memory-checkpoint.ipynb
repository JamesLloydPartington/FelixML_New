{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras.preprocessing import image_dataset_from_directory\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.utils import Sequence\n",
    "from keras.models import load_model\n",
    "from tensorflow.distribute import MirroredStrategy\n",
    "\n",
    "#initialise random generator\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define FelixDataflow classes and functions.\n",
    "\n",
    "class FelixSequence(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size, file_type):\n",
    "        \"\"\"Here self.x is a list of paths to file_type files. self.y is a\n",
    "        corresponding list of labels.\"\"\"\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.file_type = file_type\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return arrs_from_paths(batch_x, self.file_type), to_categorical(np.array(batch_y),10)\n",
    "\n",
    "def gen_paths_labels(base_path):\n",
    "    \"\"\"A generator to yield (data-paths, corresponding labels) tuples for each\n",
    "    segment of data (typically training, validation, and testing).\"\"\"\n",
    "    for segment in sorted(os.listdir(base_path)):\n",
    "        segment_path = os.path.join(base_path, segment)\n",
    "        segment_paths = []\n",
    "        segment_labels = []\n",
    "        for label in os.listdir(segment_path):\n",
    "            label_path = os.path.join(segment_path, label)\n",
    "            for crystal in os.listdir(label_path):\n",
    "                segment_paths.append(os.path.join(label_path, crystal))\n",
    "                segment_labels.append(label)\n",
    "        indexes = np.arange(len(segment_labels))\n",
    "        rng.shuffle(indexes)\n",
    "        yield [np.array(segment_paths)[indexes], np.array(list(map(int,segment_labels)))[indexes]]\n",
    "\n",
    "def arrs_from_paths(paths, file_type):\n",
    "    if file_type == \"txt\":\n",
    "        return np.array([np.loadtxt(file_name) for file_name in paths])\n",
    "    elif file_type == \"npy\":\n",
    "        return np.array([np.load(file_name)[[0],:,:] for file_name in paths])\n",
    "\n",
    "def felix_fit_new(model, batch_size, epochs, workers, AllPaths, file_type, patience):\n",
    "    #AllPaths = [[TrainingPaths, TrainingThickness], [], []]\n",
    "    \"\"\"A fit function to allow validation and test data to be supplied via a\n",
    "    generator.\"\"\"\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    \n",
    "    x = np.arange(0, epochs)\n",
    "    \n",
    "    train_seq = FelixSequence(AllPaths[0][0], AllPaths[0][1], batch_size, file_type)\n",
    "    val_seq = FelixSequence(AllPaths[1][0], AllPaths[1][1], batch_size, file_type)\n",
    "    test_seq = FelixSequence(AllPaths[2][0], AllPaths[2][1], batch_size, file_type)\n",
    "    for epoch in range(epochs):\n",
    "        print(\"-------------------------------------------------------------------------\")\n",
    "        print(\"Epoch\", epoch+1, \"/\", epochs, \": \")\n",
    "        print(\"Training: \")\n",
    "        train_hist = model.fit(x = train_seq, epochs = epoch+1, workers = workers, initial_epoch = epoch, shuffle=True)\n",
    "\n",
    "        print(\"Validation: \")\n",
    "        val_hist = model.evaluate(x = val_seq, workers = workers)       \n",
    "        epoch_loss = val_hist[0]\n",
    "        if(epoch_loss < best_val_loss):\n",
    "            #model.save(NewPath+ModelName)\n",
    "            print(\"The model improved from: \",best_val_loss, \"to: \", epoch_loss)\n",
    "            best_val_loss = epoch_loss\n",
    "            patience_i = 0\n",
    "        else:\n",
    "            patience_i+=1\n",
    "            print(\"The model did not improve, patience_i = \", patience_i)\n",
    "\n",
    "        print(\"Epoch loss: \", epoch_loss)\n",
    "        #val_hist[0][epoch] = avg_recon_loss\n",
    "        if(patience_i > patience):\n",
    "            print(\"Early Stopping, the model did not improve from: \", best_val_loss)\n",
    "            break\n",
    "\n",
    "    print(\"-------------------------------------------------------------------------\")\n",
    "    print(\"Testing: \")\n",
    "    tst_hist = model.evaluate(test_seq, workers = workers)\n",
    "    \n",
    "    return tst_hist[0]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeThicknessList(ListPaths):\n",
    "    Thickness = []\n",
    "    for i in ListPaths:\n",
    "        Thickness.append(int(i.split(\"/\")[-1].split(\".\")[0]))\n",
    "    Thickness = np.array(Thickness)\n",
    "    return(Thickness)\n",
    "\n",
    "def OpenTxt(Path):\n",
    "    with open(Path) as textFile:\n",
    "        lines = [line.split() for line in textFile]\n",
    "    List = []\n",
    "    for i in lines:\n",
    "        List.append(i[0])\n",
    "    return(List)\n",
    "\n",
    "DataPath = \"//home/ug-ml/felix-ML/classification/Classification000/DataPaths/\"\n",
    "\n",
    "TrainPath = OpenTxt(DataPath + \"Train_0p1.txt\")\n",
    "ValPath = OpenTxt(DataPath + \"Validation_0p1.txt\")\n",
    "TestPath = OpenTxt(DataPath + \"Test_0p1.txt\")\n",
    "\n",
    "TrainThickness = MakeThicknessList(TrainPath)\n",
    "ValThickness = MakeThicknessList(ValPath)\n",
    "TestThickness = MakeThicknessList(TestPath)\n",
    "\n",
    "AllPaths = [[TrainPath,TrainThickness],[ValPath,ValThickness],[TestPath,TestThickness]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All paths\n",
    "\n",
    "SaveDataPath = \"/home/ug-ml/Documents/GitHub_BigFiles/SaveFolder\" #Base directory of place you store information of models\n",
    "SaveFolderName = \"/Classifer_1\" #Will create a folder and put in information about the outcome / inputs\n",
    "ModelName = \"/Model.hdf5\"\n",
    "\n",
    "\n",
    "#Many variables\n",
    "\n",
    "#Model Variables\n",
    "input_shape = (1, 128, 128)\n",
    "\n",
    "#Hyper parameters\n",
    "learning_rate = 0.0005\n",
    "l2_regularizer = 0.0001\n",
    "loss = 'categorical_crossentropy'\n",
    "optimizer = \"RMSprop\" #Not a variable ONLY used for a note\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "ShuffleTrainData = True\n",
    "\n",
    "#Call back variables\n",
    "TrainingPatience = 5\n",
    "CheckPointMonitor = 'val_acc'\n",
    "EarlyStopMonitor = 'val_acc'\n",
    "\n",
    "#CPU variables\n",
    "CPUworkers = 16\n",
    "\n",
    "\n",
    "#List the name of the variables you want to save in a file\n",
    "VariableListName = [\"input_shape\", \n",
    "                   \"learning_rate\", \"l2_regularizer\", \"loss\", \"optimizer\", \"batch_size\", \"epochs\", \"ShuffleTrainData\",\n",
    "                   \"TrainingPatience\", \"CheckPointMonitor\", \"EarlyStopMonitor\",\n",
    "                   \"CPUworkers\"]\n",
    "\n",
    "#List the variables in the same order as VariableListName\n",
    "VariableListValues = [input_shape, \n",
    "                   learning_rate, l2_regularizer, loss, optimizer, batch_size, epochs, ShuffleTrainData,\n",
    "                   TrainingPatience, CheckPointMonitor, EarlyStopMonitor,\n",
    "                   CPUworkers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(learning_rate, l2_regularizer):\n",
    "    strategy = MirroredStrategy() #Allows multiple GPUs\n",
    "\n",
    "    with strategy.scope():\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Conv2D(128, (4, 4),\n",
    "                                         activation='relu',\n",
    "                                         data_format='channels_first',\n",
    "                                         input_shape= input_shape))\n",
    "        model.add(layers.MaxPooling2D((2, 2), data_format='channels_first'))\n",
    "        model.add(layers.Conv2D(128, (4, 4),\n",
    "                                         data_format='channels_first',\n",
    "                                         activation='relu'))\n",
    "        model.add(layers.MaxPooling2D((2, 2), data_format='channels_first'))\n",
    "        model.add(layers.Conv2D(128, (4, 4),\n",
    "                                         data_format='channels_first',\n",
    "                                         activation='relu'))\n",
    "        model.add(layers.MaxPooling2D((2, 2), data_format='channels_first'))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dropout(0.25))\n",
    "        model.add(layers.Dense(128, activation='relu',\n",
    "                               kernel_regularizer = l2(l2_regularizer)))\n",
    "\n",
    "        model.add(layers.Dense(10, activation='softmax',\n",
    "                               kernel_regularizer = l2(l2_regularizer)))\n",
    "\n",
    "        model.compile(loss = loss,\n",
    "                      optimizer = optimizers.RMSprop(learning_rate = learning_rate),\n",
    "                      metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 1 / 50 : \n",
      "Training: \n",
      "INFO:tensorflow:batch_all_reduce: 10 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 10 all-reduces with algorithm = nccl, num_packs = 1\n",
      "885/885 [==============================] - 76s 86ms/step - loss: 1.8824 - acc: 0.2885\n",
      "Validation: \n",
      "104/104 [==============================] - 7s 71ms/step - loss: 1.6904 - acc: 0.3605\n",
      "The model improved from:  inf to:  1.6904159784317017\n",
      "Epoch loss:  1.6904159784317017\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 2 / 50 : \n",
      "Training: \n",
      "Epoch 2/2\n",
      "885/885 [==============================] - 69s 78ms/step - loss: 1.5644 - acc: 0.4306\n",
      "Validation: \n",
      "104/104 [==============================] - 7s 72ms/step - loss: 1.4523 - acc: 0.4700\n",
      "The model improved from:  1.6904159784317017 to:  1.4523301124572754\n",
      "Epoch loss:  1.4523301124572754\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 3 / 50 : \n",
      "Training: \n",
      "Epoch 3/3\n",
      "885/885 [==============================] - 69s 78ms/step - loss: 1.3533 - acc: 0.5246\n",
      "Validation: \n",
      "104/104 [==============================] - 8s 79ms/step - loss: 1.3011 - acc: 0.5405\n",
      "The model improved from:  1.4523301124572754 to:  1.3010910749435425\n",
      "Epoch loss:  1.3010910749435425\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 4 / 50 : \n",
      "Training: \n",
      "Epoch 4/4\n",
      "885/885 [==============================] - 73s 82ms/step - loss: 1.2260 - acc: 0.5856 1s - loss: 1.22\n",
      "Validation: \n"
     ]
    }
   ],
   "source": [
    "prev_searched = [[0,1,2]]\n",
    "\n",
    "learning_rate = np.array([0.0005, 0.0001, 0.001, 0.002, 0.005])\n",
    "l2_regularizer = np.array([0.00005, 0.0001, 0.0005, 0.001, 0.002])\n",
    "batch_size = np.array([8, 16, 32, 64, 128])\n",
    "\n",
    "def neighbours(point):\n",
    "    dircs = np.array([[0,0,1],[0,1,0],[1,0,0],[0,0,-1],[0,-1,0],[-1,0,0]])\n",
    "    ns = dircs+point\n",
    "    return np.array([i for i in ns if (0<=i).all() and (i<5).all() and (i != prev_searched).all(axis=0).any()])\n",
    "\n",
    "z = np.array([0,1,2])\n",
    "converged = False\n",
    "best_test_loss = np.inf\n",
    "best_lr = np.nan\n",
    "best_l2_r = np.nan\n",
    "best_bs = np.nan\n",
    "\n",
    "while not converged:\n",
    "    neighs = neighbours(z)\n",
    "    #print(neighs)\n",
    "\n",
    "    lr = learning_rate[neighs[:,0]]\n",
    "    l2_r = l2_regularizer[neighs[:,1]]\n",
    "    bs = batch_size[neighs[:,2]]\n",
    "\n",
    "    step_params = np.array([lr, l2_r, bs]).T\n",
    "    \n",
    "    converged = True\n",
    "    \n",
    "    for i, param_set in enumerate(step_params):\n",
    "        prev_searched.append(neighs[i])\n",
    "        print(param_set[0])\n",
    "        \n",
    "        model = build_model(param_set[0], param_set[1])\n",
    "        \n",
    "        test_loss = felix_fit_new(model, param_set[2].astype(int), epochs, CPUworkers, AllPaths, \"npy\", TrainingPatience)\n",
    "        \n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            best_lr = param_set[0]\n",
    "            best_l2_r = param_set[1]\n",
    "            best_bs = param_set[2]\n",
    "            converged = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build model\n",
    "strategy = MirroredStrategy() #Allows multiple GPUs\n",
    "\n",
    "with strategy.scope():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(128, (4, 4),\n",
    "                                     activation='relu',\n",
    "                                     data_format='channels_first',\n",
    "                                     input_shape= input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2), data_format='channels_first'))\n",
    "    model.add(layers.Conv2D(128, (4, 4),\n",
    "                                     data_format='channels_first',\n",
    "                                     activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2), data_format='channels_first'))\n",
    "    model.add(layers.Conv2D(128, (4, 4),\n",
    "                                     data_format='channels_first',\n",
    "                                     activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2), data_format='channels_first'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    model.add(layers.Dense(128, activation='relu',\n",
    "                           kernel_regularizer = l2(l2_regularizer)))\n",
    "    \n",
    "    model.add(layers.Dense(10, activation='softmax',\n",
    "                           kernel_regularizer = l2(l2_regularizer)))\n",
    "\n",
    "    model.compile(loss = loss,\n",
    "                  optimizer = optimizers.RMSprop(learning_rate = learning_rate),\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "#Make folder to put model and history information\n",
    "try:\n",
    "    os.mkdir(NewPath)\n",
    "except:\n",
    "    print(\"Folder failed to be created, it may already exist\")\n",
    "    \n",
    "File1  = open(NewPath +\"/Parameters.txt\", \"w+\")\n",
    "if(len(VariableListName) == len(VariableListValues)):\n",
    "    for i in range(0, len(VariableListName)):\n",
    "        File1.write(VariableListName[i] + \" \" + str(VariableListValues[i]) + \"\\n\")\n",
    "    File1.close()\n",
    "else:\n",
    "    print(\"VariableListName and VariableListValues do not match up, so file can not be saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history = np.zeros(shape=(2,epochs))\n",
    "validation_history = np.zeros(shape=(2,epochs))\n",
    "test_history = [0,0]\n",
    "#print(model.metrics_names)\n",
    "felix_fit_new(model, batch_size, epochs, CPUworkers, AllPaths, \"npy\",TrainingPatience)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
