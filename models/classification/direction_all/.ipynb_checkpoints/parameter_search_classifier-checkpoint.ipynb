{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras.preprocessing import image_dataset_from_directory\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.utils import Sequence\n",
    "from keras.models import load_model\n",
    "from tensorflow.distribute import MirroredStrategy\n",
    "\n",
    "#initialise random generator\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define FelixDataflow classes and functions.\n",
    "\n",
    "class FelixSequence(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size, file_type):\n",
    "        \"\"\"Here self.x is a list of paths to file_type files. self.y is a\n",
    "        corresponding list of labels.\"\"\"\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.file_type = file_type\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return arrs_from_paths(batch_x, self.file_type), to_categorical(np.array(batch_y),10)\n",
    "\n",
    "def gen_paths_labels(base_path):\n",
    "    \"\"\"A generator to yield (data-paths, corresponding labels) tuples for each\n",
    "    segment of data (typically training, validation, and testing).\"\"\"\n",
    "    for segment in sorted(os.listdir(base_path)):\n",
    "        segment_path = os.path.join(base_path, segment)\n",
    "        segment_paths = []\n",
    "        segment_labels = []\n",
    "        for label in os.listdir(segment_path):\n",
    "            label_path = os.path.join(segment_path, label)\n",
    "            for crystal in os.listdir(label_path):\n",
    "                segment_paths.append(os.path.join(label_path, crystal))\n",
    "                segment_labels.append(label)\n",
    "        indexes = np.arange(len(segment_labels))\n",
    "        rng.shuffle(indexes)\n",
    "        yield [np.array(segment_paths)[indexes], np.array(list(map(int,segment_labels)))[indexes]]\n",
    "\n",
    "def arrs_from_paths(paths, file_type):\n",
    "    if file_type == \"txt\":\n",
    "        return np.array([np.loadtxt(file_name) for file_name in paths])\n",
    "    elif file_type == \"npy\":\n",
    "        return np.array([np.load(file_name)[[0],:,:] for file_name in paths])\n",
    "\n",
    "def felix_fit_new(model, batch_size, epochs, workers, AllPaths, file_type, patience):\n",
    "    #AllPaths = [[TrainingPaths, TrainingThickness], [], []]\n",
    "    \"\"\"A fit function to allow validation and test data to be supplied via a\n",
    "    generator.\"\"\"\n",
    "    \n",
    "    best_val_acc = -np.inf\n",
    "    \n",
    "    best_model = None\n",
    "    \n",
    "    x = np.arange(0, epochs)\n",
    "    \n",
    "    train_seq = FelixSequence(AllPaths[0][0], AllPaths[0][1], batch_size, file_type)\n",
    "    val_seq = FelixSequence(AllPaths[1][0], AllPaths[1][1], batch_size, file_type)\n",
    "    test_seq = FelixSequence(AllPaths[2][0], AllPaths[2][1], batch_size, file_type)\n",
    "    for epoch in range(epochs):\n",
    "        print(\"-------------------------------------------------------------------------\")\n",
    "        print(\"Epoch\", epoch+1, \"/\", epochs, \": \")\n",
    "        print(\"Training: \")\n",
    "        train_hist = model.fit(x = train_seq, epochs = epoch+1, workers = workers, initial_epoch = epoch, shuffle=True)\n",
    "\n",
    "        print(\"Validation: \")\n",
    "        val_hist = model.evaluate(x = val_seq, workers = workers)       \n",
    "        epoch_acc = val_hist[1]\n",
    "        if(epoch_acc > best_val_acc):\n",
    "            best_model = model\n",
    "            #model.save(NewPath+ModelName)\n",
    "            print(\"The model improved from: \",best_val_acc, \"to: \", epoch_acc)\n",
    "            best_val_acc = epoch_acc\n",
    "            patience_i = 0\n",
    "        else:\n",
    "            patience_i+=1\n",
    "            print(\"The model did not improve, patience_i = \", patience_i)\n",
    "\n",
    "        print(\"Epoch acc: \", epoch_acc)\n",
    "        #val_hist[0][epoch] = avg_recon_loss\n",
    "        if(patience_i > patience):\n",
    "            print(\"Early Stopping, the model did not improve from: \", best_val_acc)\n",
    "            break\n",
    "\n",
    "    print(\"-------------------------------------------------------------------------\")\n",
    "    print(\"Testing: \")\n",
    "    tst_hist = best_model.evaluate(test_seq, workers = workers)\n",
    "    \n",
    "    return tst_hist[1], best_model\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeThicknessList(ListPaths):\n",
    "    Thickness = []\n",
    "    for i in ListPaths:\n",
    "        Thickness.append(int(i.split(\"/\")[-1].split(\".\")[0]))\n",
    "    Thickness = np.array(Thickness)\n",
    "    return(Thickness)\n",
    "\n",
    "def OpenTxt(Path):\n",
    "    with open(Path) as textFile:\n",
    "        lines = [line.split() for line in textFile]\n",
    "    List = []\n",
    "    for i in lines:\n",
    "        List.append(i[0])\n",
    "    return(List)\n",
    "\n",
    "\n",
    "DataPath = \"//home/ug-ml/felix-ML/classification/Classification000/DataPaths/\"\n",
    "\n",
    "TrainPathAll = OpenTxt(DataPath + \"Train_0p1.txt\")\n",
    "ValPathAll = OpenTxt(DataPath + \"Validation_0p1.txt\")\n",
    "TestPathAll = OpenTxt(DataPath + \"Test_0p1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LimTrain = int(len(TrainPathAll) * 0.1)\n",
    "LimVal = int(len(ValPathAll) * 0.1)\n",
    "LimTest = int(len(TestPathAll) * 0.1)\n",
    "\n",
    "TrainPath = TrainPathAll[0:LimTrain]\n",
    "ValPath = ValPathAll[0:LimVal]\n",
    "TestPath = TestPathAll[0:LimTest]\n",
    "\n",
    "TrainThickness = MakeThicknessList(TrainPath)[0:LimTrain]\n",
    "ValThickness = MakeThicknessList(ValPath)[0:LimVal]\n",
    "TestThickness = MakeThicknessList(TestPath)[0:LimTest]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "AllPaths = [[TrainPath,TrainThickness],[ValPath,ValThickness],[TestPath,TestThickness]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All paths\n",
    "\n",
    "SaveDataPath = \"/home/ug-ml/Documents/GitHub_BigFiles/SaveFolder\" #Base directory of place you store information of models\n",
    "SaveFolderName = \"/Classifer_1\" #Will create a folder and put in information about the outcome / inputs\n",
    "ModelName = \"/Model.hdf5\"\n",
    "\n",
    "NewPath = SaveDataPath + SaveFolderName\n",
    "#Many variables\n",
    "\n",
    "#Model Variables\n",
    "input_shape = (1, 128, 128)\n",
    "\n",
    "#Hyper parameters\n",
    "learning_rate = 0.0005\n",
    "l2_regularizer = 0.0001\n",
    "loss = 'categorical_crossentropy'\n",
    "optimizer = \"RMSprop\" #Not a variable ONLY used for a note\n",
    "batch_size = 32\n",
    "epochs = 1\n",
    "ShuffleTrainData = True\n",
    "\n",
    "#Call back variables\n",
    "TrainingPatience = 5\n",
    "CheckPointMonitor = 'val_acc'\n",
    "EarlyStopMonitor = 'val_acc'\n",
    "\n",
    "#CPU variables\n",
    "CPUworkers = 16\n",
    "\n",
    "\n",
    "#List the name of the variables you want to save in a file\n",
    "VariableListName = [\"input_shape\", \n",
    "                   \"learning_rate\", \"l2_regularizer\", \"loss\", \"optimizer\", \"batch_size\", \"epochs\", \"ShuffleTrainData\",\n",
    "                   \"TrainingPatience\", \"CheckPointMonitor\", \"EarlyStopMonitor\",\n",
    "                   \"CPUworkers\"]\n",
    "\n",
    "#List the variables in the same order as VariableListName\n",
    "VariableListValues = [input_shape, \n",
    "                   learning_rate, l2_regularizer, loss, optimizer, batch_size, epochs, ShuffleTrainData,\n",
    "                   TrainingPatience, CheckPointMonitor, EarlyStopMonitor,\n",
    "                   CPUworkers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(learning_rate, l2_regularizer, dense_layer_size, kernal_size, kernal_num, opt, drop, loss_func):\n",
    "    \n",
    "    dense_layer_size = int(dense_layer_size)\n",
    "    kernal_size = int(kernal_size)\n",
    "    strategy = MirroredStrategy() #Allows multiple GPUs\n",
    "    \n",
    "    if loss_func == 0:\n",
    "        acvt = 'relu'\n",
    "    elif loss_func == 1:\n",
    "        acvt = 'sigmoid'\n",
    "    elif loss_func == 2:\n",
    "        acvt = 'tanh'\n",
    "        \n",
    "    with strategy.scope():\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Conv2D(kernal_num, (kernal_size, kernal_size),\n",
    "                                         activation=acvt,\n",
    "                                         data_format='channels_first',\n",
    "                                         input_shape= input_shape))\n",
    "        model.add(layers.MaxPooling2D((2, 2), data_format='channels_first'))\n",
    "        model.add(layers.Conv2D(kernal_num, (kernal_size, kernal_size),\n",
    "                                         data_format='channels_first',\n",
    "                                         activation=acvt))\n",
    "        model.add(layers.MaxPooling2D((2, 2), data_format='channels_first'))\n",
    "        model.add(layers.Conv2D(kernal_num, (kernal_size, kernal_size),\n",
    "                                         data_format='channels_first',\n",
    "                                         activation=acvt))\n",
    "        model.add(layers.MaxPooling2D((2, 2), data_format='channels_first'))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dropout(drop))\n",
    "        model.add(layers.Dense(dense_layer_size, activation=acvt,\n",
    "                               kernel_regularizer = l2(l2_regularizer)))\n",
    "\n",
    "        model.add(layers.Dense(10, activation='softmax',\n",
    "                               kernel_regularizer = l2(l2_regularizer)))\n",
    "        if opt == 0:\n",
    "            model.compile(loss = loss,\n",
    "                          optimizer = optimizers.RMSprop(learning_rate = learning_rate),\n",
    "                          metrics=['acc'])\n",
    "        elif opt == 1:\n",
    "            model.compile(loss = loss,\n",
    "                          optimizer = optimizers.SGD(learning_rate = learning_rate),\n",
    "                          metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATING OVER:\n",
      "[[2 2 1 1 3 0 2 0]\n",
      " [1 3 1 1 3 0 2 0]\n",
      " [1 2 2 1 3 0 2 0]\n",
      " [1 2 1 2 3 0 2 0]\n",
      " [1 2 1 1 4 0 2 0]\n",
      " [1 2 1 1 3 1 2 0]\n",
      " [1 2 1 1 3 0 3 0]\n",
      " [1 2 1 1 3 0 2 1]\n",
      " [0 2 1 1 3 0 2 0]\n",
      " [1 1 1 1 3 0 2 0]\n",
      " [1 2 0 1 3 0 2 0]\n",
      " [1 2 1 0 3 0 2 0]\n",
      " [1 2 1 1 2 0 2 0]\n",
      " [1 2 1 1 3 0 1 0]] 14\n",
      "Parameter set  1  of  14 +++++++++++++++++++++++++++++++++++++++++++++++++++.\n",
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 1 / 1 : \n",
      "Training: \n",
      "WARNING:tensorflow:From /home/ug-ml/felix-ML/env/lib/python3.6/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " Default MaxPoolingOp only supports NHWC on device type CPU\n\t [[node sequential/max_pooling2d/MaxPool (defined at usr/lib/python3.6/threading.py:916) ]] [Op:__inference_train_function_1343]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-19064b8f98ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0mparameter_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_searched\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-19064b8f98ee>\u001b[0m in \u001b[0;36mparameter_search\u001b[0;34m(parameters, z, prev_searched, learning_rate)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfelix_fit_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCPUworkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAllPaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingPatience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mprev_searched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_searched\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-723e3ab08b7f>\u001b[0m in \u001b[0;36mfelix_fit_new\u001b[0;34m(model, batch_size, epochs, workers, AllPaths, file_type, patience)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\": \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mtrain_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/felix-ML/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/felix-ML/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/felix-ML/env/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/felix-ML/env/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/felix-ML/env/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/felix-ML/env/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/felix-ML/env/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/felix-ML/env/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/felix-ML/env/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  Default MaxPoolingOp only supports NHWC on device type CPU\n\t [[node sequential/max_pooling2d/MaxPool (defined at usr/lib/python3.6/threading.py:916) ]] [Op:__inference_train_function_1343]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "parameters = {\"l2_regulariser\": np.array([0.00005, 0.0001, 0.0005, 0.001, 0.002]),\n",
    "             \"batch_size\": np.array([8, 16, 32, 64, 128]),\n",
    "             \"dense_layer_size\":np.array([32, 64, 128, 256, 512]),\n",
    "             \"kernal_size\": np.array([2, 4, 6, 8, 10]),\n",
    "             \"kernal_num\": np.array([16, 32, 64, 128, 256]),\n",
    "             \"optimisers\": np.array([0,1]),\n",
    "             \"dropout\": np.array([0.1, 0.2, 0.25, 0.3, 0.4]),\n",
    "             \"loss_func\": np.array([0,1,2])}\n",
    "    \n",
    "z = np.array([1,2,1,1,3,0,2,0])\n",
    "prev_searched = np.array([[1,2,1,1,3,0,2,0]])\n",
    "\n",
    "def neighbours(point, dircs):\n",
    "    ns = dircs+point\n",
    "    #print(dircs, point, prev_searched)\n",
    "    return np.array([i for i in ns if (0<=i).all() and (i<5).all() and (0<=i[5]<2) and (0<=i[7]<3) and not (i == prev_searched).all(axis=1).any()]).astype(int)\n",
    "\n",
    "def parameter_search(parameters, z, prev_searched, learning_rate):\n",
    "    \n",
    "    num_params = len(parameters)\n",
    "    dircs = np.zeros(shape = (2*num_params, num_params))\n",
    "    for i, dirc in enumerate(dircs):\n",
    "        if i < num_params:\n",
    "            dirc[i] = 1\n",
    "        else:\n",
    "            dirc[i-num_params] = -1\n",
    "    \n",
    "    best_params = {}\n",
    "    for key in parameters:\n",
    "        best_params[key] = np.nan\n",
    "\n",
    "    best_metrics = {\"test_loss\": np.inf, \"test_acc\": -np.inf}\n",
    "\n",
    "    best_hist = {\"train_acc\": np.zeros(shape = epochs),\n",
    "                \"train_loss\": np.zeros(shape=epochs),\n",
    "                \"val_acc\": np.zeros(shape = epochs),\n",
    "                \"val_loss\": np.zeros(shape=epochs)}\n",
    "    \n",
    "    converged = False\n",
    "        \n",
    "    while not converged:\n",
    "        print(\"ITERATING OVER:\")\n",
    "        neighs = neighbours(z, dircs)\n",
    "        num_neighs = neighs.shape[0]\n",
    "        print(neighs, num_neighs)\n",
    "\n",
    "        if neighs.size == 0:\n",
    "            print(\"No new neighbours available. Saving best model and parameter set so far.\")\n",
    "            best_model.save(NewPath+ModelName)\n",
    "            converged = True\n",
    "            break\n",
    "\n",
    "        step_params = np.array([parameters[key][neighs[:,j]] for j, key in enumerate(parameters)]).T\n",
    "        #print(step_params)\n",
    "                \n",
    "\n",
    "        converged = True\n",
    "\n",
    "        for i, param_set in enumerate(step_params):\n",
    "            print(\"Parameter set \", i+1,\" of \", num_neighs,\"+++++++++++++++++++++++++++++++++++++++++++++++++++.\")\n",
    "\n",
    "            model = build_model(learning_rate, param_set[0], param_set[2], param_set[3], param_set[4], param_set[5], param_set[6], param_set[7])\n",
    "\n",
    "            test_acc, best_model = felix_fit_new(model, param_set[1].astype(int), epochs, CPUworkers, AllPaths, \"npy\", TrainingPatience)\n",
    "\n",
    "            prev_searched = np.append(prev_searched, neighs[i].reshape(1,num_params), axis=0)\n",
    "\n",
    "            if test_acc > best_metrics[\"test_acc\"]:\n",
    "                \n",
    "                best_metrics[\"test_acc\"] = test_acc\n",
    "                \n",
    "                for i, key in enumerate(best_params):\n",
    "                    best_params[key] = param_set[i]\n",
    "\n",
    "                z = neighs[i]\n",
    "                \n",
    "                best_model.save(NewPath+ModelName)\n",
    "                converged = False\n",
    "        print(\"best params set:\" )\n",
    "        print(best_params)\n",
    "                \n",
    "parameter_search(parameters, z, prev_searched, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATING OVER:\n",
      "[[0 1 3]\n",
      " [0 2 2]\n",
      " [1 1 2]\n",
      " [0 1 1]\n",
      " [0 0 2]]\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 1 / 1 : \n",
      "Training: \n",
      "89/89 [==============================] - 7s 83ms/step - loss: 2.0421 - acc: 0.2734\n",
      "Validation: \n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.5310 - acc: 0.3549\n",
      "The model improved from:  inf to:  2.5309948921203613\n",
      "Epoch loss:  2.5309948921203613\n",
      "-------------------------------------------------------------------------\n",
      "Testing: \n",
      "6/6 [==============================] - 0s 20ms/step - loss: 1.9448 - acc: 0.3724\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 1 / 1 : \n",
      "Training: \n",
      "177/177 [==============================] - 7s 41ms/step - loss: 1.9997 - acc: 0.3010\n",
      "Validation: \n",
      "21/21 [==============================] - 1s 26ms/step - loss: 2.6480 - acc: 0.4045\n",
      "The model improved from:  inf to:  2.648041009902954\n",
      "Epoch loss:  2.648041009902954\n",
      "-------------------------------------------------------------------------\n",
      "Testing: \n",
      "11/11 [==============================] - 0s 14ms/step - loss: 1.8224 - acc: 0.4414\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 1 / 1 : \n",
      "Training: \n",
      "177/177 [==============================] - 6s 36ms/step - loss: 1.9428 - acc: 0.3107\n",
      "Validation: \n",
      "21/21 [==============================] - 0s 18ms/step - loss: 2.7648 - acc: 0.3955\n",
      "The model improved from:  inf to:  2.764773368835449\n",
      "Epoch loss:  2.764773368835449\n",
      "-------------------------------------------------------------------------\n",
      "Testing: \n",
      "11/11 [==============================] - 0s 18ms/step - loss: 2.0704 - acc: 0.4324\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 1 / 1 : \n",
      "Training: \n",
      "354/354 [==============================] - 10s 27ms/step - loss: 1.8249 - acc: 0.3443\n",
      "Validation: \n",
      "42/42 [==============================] - 1s 19ms/step - loss: 3.5550 - acc: 0.4195\n",
      "The model improved from:  inf to:  3.55496883392334\n",
      "Epoch loss:  3.55496883392334\n",
      "-------------------------------------------------------------------------\n",
      "Testing: \n",
      "21/21 [==============================] - 0s 16ms/step - loss: 2.4232 - acc: 0.4685\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 1 / 1 : \n",
      "Training: \n",
      "177/177 [==============================] - 6s 36ms/step - loss: 1.8983 - acc: 0.3507\n",
      "Validation: \n",
      "21/21 [==============================] - 1s 25ms/step - loss: 2.8816 - acc: 0.4060\n",
      "The model improved from:  inf to:  2.8816285133361816\n",
      "Epoch loss:  2.8816285133361816\n",
      "-------------------------------------------------------------------------\n",
      "Testing: \n",
      "11/11 [==============================] - 0s 15ms/step - loss: 2.0423 - acc: 0.4655\n",
      "ITERATING OVER:\n",
      "[[0 2 3]\n",
      " [0 3 2]\n",
      " [1 2 2]\n",
      " [0 2 1]]\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 1 / 1 : \n",
      "Training: \n",
      "89/89 [==============================] - 7s 78ms/step - loss: 2.1215 - acc: 0.2598\n",
      "Validation: \n",
      "11/11 [==============================] - 0s 25ms/step - loss: 2.8650 - acc: 0.3459\n",
      "The model improved from:  inf to:  2.8649563789367676\n",
      "Epoch loss:  2.8649563789367676\n",
      "-------------------------------------------------------------------------\n",
      "Testing: \n",
      "6/6 [==============================] - 0s 17ms/step - loss: 2.1361 - acc: 0.3724\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 1 / 1 : \n",
      "Training: \n",
      "177/177 [==============================] - 6s 34ms/step - loss: 2.1102 - acc: 0.2833\n",
      "Validation: \n",
      "21/21 [==============================] - 1s 25ms/step - loss: 3.0132 - acc: 0.3910\n",
      "The model improved from:  inf to:  3.0132031440734863\n",
      "Epoch loss:  3.0132031440734863\n",
      "-------------------------------------------------------------------------\n",
      "Testing: \n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.2549 - acc: 0.4084\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 1 / 1 : \n",
      "Training: \n",
      "177/177 [==============================] - 6s 36ms/step - loss: 2.1055 - acc: 0.3017\n",
      "Validation: \n",
      "21/21 [==============================] - 1s 25ms/step - loss: 2.3431 - acc: 0.3368\n",
      "The model improved from:  inf to:  2.3431174755096436\n",
      "Epoch loss:  2.3431174755096436\n",
      "-------------------------------------------------------------------------\n",
      "Testing: \n",
      "11/11 [==============================] - 0s 15ms/step - loss: 1.9427 - acc: 0.3453\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 1 / 1 : \n",
      "Training: \n",
      "354/354 [==============================] - 8s 23ms/step - loss: 1.9091 - acc: 0.3980\n",
      "Validation: \n",
      "42/42 [==============================] - 1s 17ms/step - loss: 2.3930 - acc: 0.4647\n",
      "The model improved from:  inf to:  2.393003225326538\n",
      "Epoch loss:  2.393003225326538\n",
      "-------------------------------------------------------------------------\n",
      "Testing: \n",
      "21/21 [==============================] - 0s 14ms/step - loss: 1.8335 - acc: 0.4985\n"
     ]
    }
   ],
   "source": [
    "prev_searched = np.array([[0,1,2]])\n",
    "\n",
    "learning_rate = np.array([0.0005, 0.0001, 0.001, 0.002, 0.005])\n",
    "l2_regularizer = np.array([0.00005, 0.0001, 0.0005, 0.001, 0.002])\n",
    "batch_size = np.array([8, 16, 32, 64, 128])\n",
    "\n",
    "def neighbours(point):\n",
    "    dircs = np.array([[0,0,1],[0,1,0],[1,0,0],[0,0,-1],[0,-1,0],[-1,0,0]])\n",
    "    ns = dircs+point\n",
    "    return np.array([i for i in ns if (0<=i).all() and (i<5).all() and not (i == prev_searched).all(axis=1).any()])\n",
    "\n",
    "\n",
    "z = np.array([0,1,2])\n",
    "converged = False\n",
    "best_test_loss = np.inf\n",
    "best_lr = np.nan\n",
    "best_l2_r = np.nan\n",
    "best_bs = np.nan\n",
    "\n",
    "while not converged:\n",
    "    print(\"ITERATING OVER:\")\n",
    "    neighs = neighbours(z)\n",
    "    print(neighs)\n",
    "    #print(neighs)\n",
    "    if neighs.size == 0:\n",
    "        print(\"No new neighbours available. Saving best model and parameter set so far.\")\n",
    "        np.save(NewPath+\"/parameter_search.npy\", np.array(best_lr,best_l2_r,best_bs))\n",
    "        best_model.save(NewPath+ModelName)\n",
    "        converged = True\n",
    "        break\n",
    "        \n",
    "\n",
    "    lr = learning_rate[neighs[:,0]]\n",
    "    l2_r = l2_regularizer[neighs[:,1]]\n",
    "    bs = batch_size[neighs[:,2]]\n",
    "\n",
    "    step_params = np.array([lr, l2_r, bs]).T\n",
    "    \n",
    "    converged = True\n",
    "    \n",
    "    for i, param_set in enumerate(step_params):\n",
    "        \n",
    "        model = build_model(param_set[0], param_set[1])\n",
    "        \n",
    "        test_loss, best_model = felix_fit_new(model, param_set[2].astype(int), epochs, CPUworkers, AllPaths, \"npy\", TrainingPatience)\n",
    "        \n",
    "        prev_searched = np.append(prev_searched, neighs[i].reshape(1,3), axis=0)\n",
    "        \n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            best_lr = param_set[0]\n",
    "            best_l2_r = param_set[1]\n",
    "            best_bs = param_set[2]\n",
    "            \n",
    "            z = neighs[i]\n",
    "            \n",
    "            np.save(NewPath+\"/parameter_search.npy\",param_set)\n",
    "            best_model.save(NewPath+ModelName)\n",
    "            converged = False\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005 0.0005 32.0\n",
      "1.822420358657837\n"
     ]
    }
   ],
   "source": [
    "print(best_lr,best_l2_r,best_bs)\n",
    "print(best_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 10ms/step - loss: 1.6834 - acc: 0.6216\n"
     ]
    }
   ],
   "source": [
    "test_seq = FelixSequence(AllPaths[2][0], AllPaths[2][1], best_bs.astype(int), \"npy\")\n",
    "tst_hist = best_model.evaluate(test_seq, workers = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build model\n",
    "strategy = MirroredStrategy() #Allows multiple GPUs\n",
    "\n",
    "with strategy.scope():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(128, (4, 4),\n",
    "                                     activation='relu',\n",
    "                                     data_format='channels_first',\n",
    "                                     input_shape= input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2), data_format='channels_first'))\n",
    "    model.add(layers.Conv2D(128, (4, 4),\n",
    "                                     data_format='channels_first',\n",
    "                                     activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2), data_format='channels_first'))\n",
    "    model.add(layers.Conv2D(128, (4, 4),\n",
    "                                     data_format='channels_first',\n",
    "                                     activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2), data_format='channels_first'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    model.add(layers.Dense(128, activation='relu',\n",
    "                           kernel_regularizer = l2(l2_regularizer)))\n",
    "    \n",
    "    model.add(layers.Dense(10, activation='softmax',\n",
    "                           kernel_regularizer = l2(l2_regularizer)))\n",
    "\n",
    "    model.compile(loss = loss,\n",
    "                  optimizer = optimizers.RMSprop(learning_rate = learning_rate),\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "#Make folder to put model and history information\n",
    "try:\n",
    "    os.mkdir(NewPath)\n",
    "except:\n",
    "    print(\"Folder failed to be created, it may already exist\")\n",
    "    \n",
    "File1  = open(NewPath +\"/Parameters.txt\", \"w+\")\n",
    "if(len(VariableListName) == len(VariableListValues)):\n",
    "    for i in range(0, len(VariableListName)):\n",
    "        File1.write(VariableListName[i] + \" \" + str(VariableListValues[i]) + \"\\n\")\n",
    "    File1.close()\n",
    "else:\n",
    "    print(\"VariableListName and VariableListValues do not match up, so file can not be saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history = np.zeros(shape=(2,epochs))\n",
    "validation_history = np.zeros(shape=(2,epochs))\n",
    "test_history = [0,0]\n",
    "#print(model.metrics_names)\n",
    "felix_fit_new(model, batch_size, epochs, CPUworkers, AllPaths, \"npy\",TrainingPatience)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
