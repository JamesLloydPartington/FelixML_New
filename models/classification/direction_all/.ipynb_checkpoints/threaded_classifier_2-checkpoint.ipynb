{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras.preprocessing import image_dataset_from_directory\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.utils import Sequence\n",
    "from keras.models import load_model\n",
    "from tensorflow.distribute import MirroredStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise random generator\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define FelixDataflow classes and functions.\n",
    "\n",
    "class FelixSequence(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size, file_type):\n",
    "        \"\"\"Here self.x is a list of paths to file_type files. self.y is a\n",
    "        corresponding list of labels.\"\"\"\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.file_type = file_type\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return arrs_from_paths(batch_x, self.file_type), to_categorical(np.array(batch_y),10)\n",
    "\n",
    "def gen_paths_labels(base_path):\n",
    "    \"\"\"A generator to yield (data-paths, corresponding labels) tuples for each\n",
    "    segment of data (typically training, validation, and testing).\"\"\"\n",
    "    for segment in sorted(os.listdir(base_path)):\n",
    "        segment_path = os.path.join(base_path, segment)\n",
    "        segment_paths = []\n",
    "        segment_labels = []\n",
    "        for label in os.listdir(segment_path):\n",
    "            label_path = os.path.join(segment_path, label)\n",
    "            for crystal in os.listdir(label_path):\n",
    "                segment_paths.append(os.path.join(label_path, crystal))\n",
    "                segment_labels.append(label)\n",
    "        indexes = np.arange(len(segment_labels))\n",
    "        rng.shuffle(indexes)\n",
    "        yield [np.array(segment_paths)[indexes], np.array(list(map(int,segment_labels)))[indexes]]\n",
    "\n",
    "def arrs_from_paths(paths, file_type):\n",
    "    if file_type == \"txt\":\n",
    "        return np.array([np.loadtxt(file_name) for file_name in paths])\n",
    "    elif file_type == \"npy\":\n",
    "        #return np.array([np.load(file_name)[[0],:,:] for file_name in paths])\n",
    "        return np.array([np.load(file_name) for file_name in paths])\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def felix_fit_new(model, batch_size, epochs, workers, callbacks, AllPaths, file_type, train_history, val_history, patience, test_history, Monitor):\n",
    "    #AllPaths = [[TrainingPaths, TrainingThickness], [], []]\n",
    "    \"\"\"A fit function to allow validation and test data to be supplied via a\n",
    "    generator.\"\"\"\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    best_val_acc = 0\n",
    "    \n",
    "    x = np.arange(0, epochs)\n",
    "    \n",
    "    train_seq = FelixSequence(AllPaths[0][0], AllPaths[0][1], batch_size, file_type)\n",
    "    val_seq = FelixSequence(AllPaths[1][0], AllPaths[1][1], batch_size, file_type)\n",
    "    test_seq = FelixSequence(AllPaths[2][0], AllPaths[2][1], batch_size, file_type)\n",
    "    for epoch in range(epochs):\n",
    "        print(\"-------------------------------------------------------------------------\")\n",
    "        print(\"Epoch\", epoch+1, \"/\", epochs, \": \")\n",
    "        print(\"Training: \")\n",
    "        train_hist = model.fit(x = train_seq, epochs = epoch+1, workers = workers, initial_epoch = epoch, shuffle=True)\n",
    "        train_history[0][epoch] = train_hist.history[\"loss\"][0]\n",
    "        train_history[1][epoch] = train_hist.history[\"acc\"][0]\n",
    "        print(\"Validation: \")\n",
    "        val_hist = model.evaluate(x = val_seq, workers = workers, callbacks = callbacks)\n",
    "        #print(val_hist)\n",
    "        val_history[0][epoch] = val_hist[0]\n",
    "        val_history[1][epoch] = val_hist[1]\n",
    "        \n",
    "        plt.plot(x, val_history[1])\n",
    "        plt.show()\n",
    "        \n",
    "        epoch_loss = val_hist[0]\n",
    "        epoch_acc = val_hist[1]\n",
    "        \n",
    "        \n",
    "        if(Monitor == \"loss\"):\n",
    "            if(epoch_loss < best_val_loss):\n",
    "                model.save(NewPath+ModelName)\n",
    "                print(\"The model loss improved from: \",best_val_loss, \"to: \", epoch_loss)\n",
    "                best_val_loss = epoch_loss\n",
    "                patience_i = 0\n",
    "            else:\n",
    "                patience_i+=1\n",
    "                print(\"The model did not improve, patience_i = \", patience_i)\n",
    "\n",
    "            #val_hist[0][epoch] = avg_recon_loss\n",
    "            if(patience_i > patience):\n",
    "                print(\"Early Stopping, the model did not improve from a loss: \", best_val_loss)\n",
    "                break\n",
    "        \n",
    "        elif(Monitor == \"acc\"):\n",
    "            if(best_val_acc < epoch_acc):\n",
    "                model.save(NewPath+ModelName)\n",
    "                print(\"The model accuracy improved from: \",best_val_acc, \"to: \", epoch_acc)\n",
    "                best_val_acc = epoch_acc\n",
    "                patience_i = 0\n",
    "            else:\n",
    "                patience_i+=1\n",
    "                print(\"The model accuracy did not improve, patience_i = \", patience_i)\n",
    "\n",
    "            #val_hist[0][epoch] = avg_recon_loss\n",
    "            if(patience_i > patience):\n",
    "                print(\"Early Stopping, the model did not improve from an accuracy: \", best_val_loss)\n",
    "                break\n",
    "\n",
    "    print(\"-------------------------------------------------------------------------\")\n",
    "    print(\"Testing: \")\n",
    "    tst_hist = model.evaluate(test_seq)\n",
    "    test_history[0] = tst_hist[0]\n",
    "    test_history[1] = tst_hist[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All paths\n",
    "\n",
    "SaveDataPath = \"/home/ug-ml/Documents/GitHub_BigFiles/SaveFolder\" #Base directory of place you store information of models\n",
    "SaveFolderName = \"/Classifer_1\" #Will create a folder and put in information about the outcome / inputs\n",
    "ModelName = \"/Model.hdf5\"\n",
    "\n",
    "\n",
    "#Many variables\n",
    "\n",
    "#Model Variables\n",
    "input_shape = (15, 128, 128)\n",
    "\n",
    "#Hyper parameters\n",
    "learning_rate = 0.0005\n",
    "l2_regularizer = 0.0001\n",
    "loss = 'categorical_crossentropy'\n",
    "optimizer = \"RMSprop\" #Not a variable ONLY used for a note\n",
    "batch_size = 16\n",
    "epochs = 50\n",
    "ShuffleTrainData = True\n",
    "\n",
    "#Call back variables\n",
    "TrainingPatience = 5\n",
    "CheckPointMonitor = 'val_acc'\n",
    "EarlyStopMonitor = 'val_acc'\n",
    "\n",
    "#CPU variables\n",
    "CPUworkers = 16\n",
    "\n",
    "\n",
    "#List the name of the variables you want to save in a file\n",
    "VariableListName = [\"input_shape\", \n",
    "                   \"learning_rate\", \"l2_regularizer\", \"loss\", \"optimizer\", \"batch_size\", \"epochs\", \"ShuffleTrainData\",\n",
    "                   \"TrainingPatience\", \"CheckPointMonitor\", \"EarlyStopMonitor\",\n",
    "                   \"CPUworkers\"]\n",
    "\n",
    "#List the variables in the same order as VariableListName\n",
    "VariableListValues = [input_shape, \n",
    "                   learning_rate, l2_regularizer, loss, optimizer, batch_size, epochs, ShuffleTrainData,\n",
    "                   TrainingPatience, CheckPointMonitor, EarlyStopMonitor,\n",
    "                   CPUworkers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder failed to be created, it may already exist\n"
     ]
    }
   ],
   "source": [
    "#Early stopping and check points\n",
    "\n",
    "EarlyStop = EarlyStopping(monitor = EarlyStopMonitor,\n",
    "                          mode = 'min',\n",
    "                          verbose = 1,\n",
    "                          patience = TrainingPatience)\n",
    "\n",
    "NewPath = SaveDataPath + SaveFolderName\n",
    "Checkpoint = ModelCheckpoint(NewPath + ModelName, #Save path\n",
    "                             monitor = CheckPointMonitor,\n",
    "                             verbose = 1,\n",
    "                             save_best_only = True,\n",
    "                             mode = 'auto',\n",
    "                             save_freq = 'epoch')\n",
    "\n",
    "\n",
    "#Make folder to put model and history information\n",
    "try:\n",
    "    os.mkdir(NewPath)\n",
    "except:\n",
    "    print(\"Folder failed to be created, it may already exist\")\n",
    "    \n",
    "File1  = open(NewPath +\"/Parameters.txt\", \"w+\")\n",
    "if(len(VariableListName) == len(VariableListValues)):\n",
    "    for i in range(0, len(VariableListName)):\n",
    "        File1.write(VariableListName[i] + \" \" + str(VariableListValues[i]) + \"\\n\")\n",
    "    File1.close()\n",
    "else:\n",
    "    print(\"VariableListName and VariableListValues do not match up, so file can not be saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 48, 125, 125)      11568     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 48, 62, 62)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 48, 59, 59)        36912     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 48, 29, 29)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 48, 26, 26)        36912     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 48, 13, 13)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 8112)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8112)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 48)                389424    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                490       \n",
      "=================================================================\n",
      "Total params: 475,306\n",
      "Trainable params: 475,306\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Build model\n",
    "strategy = MirroredStrategy() #Allows multiple GPUs\n",
    "\n",
    "with strategy.scope():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(48, (4, 4),\n",
    "                                     activation='relu',\n",
    "                                     data_format='channels_first',\n",
    "                                     input_shape= input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2), data_format='channels_first'))\n",
    "    model.add(layers.Conv2D(48, (4, 4),\n",
    "                                     data_format='channels_first',\n",
    "                                     activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2), data_format='channels_first'))\n",
    "    model.add(layers.Conv2D(48, (4, 4),\n",
    "                                     data_format='channels_first',\n",
    "                                     activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2), data_format='channels_first'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(48, activation='relu',\n",
    "                           kernel_regularizer = l2(l2_regularizer)))\n",
    "    \n",
    "    model.add(layers.Dense(10, activation='softmax',\n",
    "                           kernel_regularizer = l2(l2_regularizer)))\n",
    "\n",
    "    model.compile(loss = loss,\n",
    "                  optimizer = optimizers.RMSprop(learning_rate = learning_rate),\n",
    "                  metrics=['acc'])\n",
    "\n",
    "#Save summary of model\n",
    "with open(NewPath + '/summary.txt','w') as fh:\n",
    "    model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeThicknessList(ListPaths):\n",
    "    Thickness = []\n",
    "    for i in ListPaths:\n",
    "        Thickness.append(int(i.split(\"/\")[-1].split(\".\")[0]))\n",
    "    Thickness = np.array(Thickness)\n",
    "    return(Thickness)\n",
    "\n",
    "def OpenTxt(Path):\n",
    "    with open(Path) as textFile:\n",
    "        lines = [line.split() for line in textFile]\n",
    "    List = []\n",
    "    for i in lines:\n",
    "        List.append(i[0])\n",
    "    return(List)\n",
    "\n",
    "DataPath = \"/home/ug-ml/felix-ML/classification/Classification15/DataPaths/\"\n",
    "\n",
    "TrainPath = OpenTxt(DataPath + \"Train_0p1.txt\")\n",
    "ValPath = OpenTxt(DataPath + \"Validation_0p1.txt\")\n",
    "TestPath = OpenTxt(DataPath + \"Test_0p1.txt\")\n",
    "\n",
    "TrainThickness = MakeThicknessList(TrainPath)\n",
    "ValThickness = MakeThicknessList(ValPath)\n",
    "TestThickness = MakeThicknessList(TestPath)\n",
    "\n",
    "AllPaths = [[TrainPath,TrainThickness],[ValPath,ValThickness],[TestPath,TestThickness]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------\n",
      "Epoch 1 / 50 : \n",
      "Training: \n",
      "INFO:tensorflow:batch_all_reduce: 10 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 10 all-reduces with algorithm = nccl, num_packs = 1\n",
      " 256/3537 [=>............................] - ETA: 4:54 - loss: 2.0785 - acc: 0.1992"
     ]
    }
   ],
   "source": [
    "training_history = np.zeros(shape=(2,epochs))\n",
    "validation_history = np.zeros(shape=(2,epochs))\n",
    "test_history = [0,0]\n",
    "#print(model.metrics_names)\n",
    "felix_fit_new(model, batch_size, epochs, CPUworkers, [EarlyStop, Checkpoint], AllPaths, \"npy\", training_history, validation_history, TrainingPatience, test_history, \"acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(NewPath+ModelName)\n",
    "np.save(NewPath+\"/training_history.npy\", training_history)\n",
    "np.save(NewPath+\"/validation_history.npy\", validation_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
