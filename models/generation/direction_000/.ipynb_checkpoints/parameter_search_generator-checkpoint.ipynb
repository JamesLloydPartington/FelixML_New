{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, layers, backend, Model, losses, datasets, models, metrics, optimizers, initializers\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "\n",
    "#initialise random generator\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FelixSequence(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        \"\"\"Here self.x is a list of paths to .npy input files. self.y is a\n",
    "        corresponding list of paths to .npy output files.\"\"\"\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        #print(np.array([np.load(file_name) for file_name in batch_x]).shape, np.array([np.load(file_name) for file_name in batch_y]).shape)\n",
    "        return np.array([np.reshape(np.load(file_name), (128, 128, 1)) for file_name in batch_x]), np.array([np.reshape(np.load(file_name), (128, 128, 1)) for file_name in batch_y])\n",
    "    \n",
    "\n",
    "def gen_paths_labels(base_path = \"D:\\\\Uni Work\\\\Masters Project\\\\electron_dists\\\\Data\\\\VAE_000_1\\\\Data\"):\n",
    "    \"\"\"A generator to yield (data-paths, corresponding labels) tuples for each\n",
    "    segment of data (typically training, validation, and testing).\"\"\"\n",
    "    for segment in sorted(os.listdir(base_path)):\n",
    "        segment_path = os.path.join(base_path, segment)\n",
    "        input_paths = []\n",
    "        output_paths = []\n",
    "        for crystal in sorted(os.listdir(segment_path)):\n",
    "            crystal_path = os.path.join(segment_path, crystal)\n",
    "            files = sorted(os.listdir(crystal_path))\n",
    "            input_paths.append(os.path.join(crystal_path, files[0]))\n",
    "            output_paths.append(os.path.join(crystal_path, files[1]))\n",
    "        yield [input_paths, output_paths]\n",
    "\n",
    "def gen_paths_fromfile(Path):\n",
    "    Paths = []\n",
    "    with open(Path) as textFile:\n",
    "        lines = [line.split() for line in textFile]\n",
    "    for i in lines:\n",
    "        Paths.append(i[0])\n",
    "        \n",
    "    Paths = np.array(Paths, dtype = \"object\")\n",
    "    return(Paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 16\n",
    "#lap = tf.compat.v1.distributions.Laplace(0.0,1.0)\n",
    "\"\"\"\n",
    "## Create a sampling layer\n",
    "\"\"\"\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "    def __init__(self, gamma = 1, **kwargs):\n",
    "        super(Sampling, self).__init__(**kwargs)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        \n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon * self.gamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ZMCC(Image1, Image2):\n",
    "    sd1 = tf.math.reduce_std(Image1, axis = (1,2))\n",
    "    mean1 = tf.math.reduce_mean(Image1, axis = (1,2), keepdims = True)\n",
    "    \n",
    "    sd2 = tf.math.reduce_std(Image2, axis = (1,2))\n",
    "    mean2 = tf.math.reduce_mean(Image2, axis = (1,2), keepdims = True)\n",
    "\n",
    "    img1 = (Image1 - mean1)\n",
    "    img2 = (Image2 - mean2)\n",
    "    img = img1*img2\n",
    "\n",
    "    zmcc = (1 - (1 / (128 * 128 * sd1 * sd2)) *  tf.reduce_sum(img, axis=(1,2)))\n",
    "    return(zmcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 64, 64, 16)        1040      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 16)        16400     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 91)                1491035   \n",
      "_________________________________________________________________\n",
      "z_mean (Dense)               (None, 16)                1472      \n",
      "_________________________________________________________________\n",
      "z_log_var (Dense)            (None, 16)                1472      \n",
      "_________________________________________________________________\n",
      "sampling_1 (Sampling)        (None, 16)                0         \n",
      "=================================================================\n",
      "Total params: 1,511,419\n",
      "Trainable params: 1,511,419\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Build the encoder\n",
    "\"\"\"\n",
    "\n",
    "Num_Kernals = 16\n",
    "Size_Kernals = 8\n",
    "\n",
    "class Encoder(Model):\n",
    "    def __init__(self, gamma = 0, Size_Kernals=8, Num_Kernals=16, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "\n",
    "        self.Conv1 = layers.Conv2D(Num_Kernals, kernel_size = (Size_Kernals, Size_Kernals), activation=\"relu\", strides=2, padding=\"same\")\n",
    "        self.Conv2 = layers.Conv2D(Num_Kernals, kernel_size = (Size_Kernals, Size_Kernals), activation=\"relu\", strides=2, padding=\"same\")\n",
    "\n",
    "        self.flat = layers.Flatten()\n",
    "\n",
    "        self.DenseParam_Encode = 1500000\n",
    "        self.DenseNeurons_Encode = int(self.DenseParam_Encode / 16400)\n",
    "\n",
    "        self.dense = layers.Dense(self.DenseNeurons_Encode, activation=\"relu\", kernel_regularizer = l2(0.1))\n",
    "        self.z_mean = layers.Dense(latent_dim, name=\"z_mean\")\n",
    "        self.z_log_var = layers.Dense(latent_dim, name=\"z_log_var\", kernel_initializer='zeros', bias_initializer='zeros')\n",
    "        self.sampling = Sampling(gamma=gamma)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "\n",
    "        x = self.Conv1(inputs)\n",
    "        x = self.Conv2(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.dense(x)\n",
    "        z_mean = self.z_mean(x)\n",
    "        z_log_var = self.z_log_var(x)\n",
    "        z = self.sampling([z_mean, z_log_var])\n",
    "        return z_mean, z_log_var, z\n",
    "    \n",
    "encoder = Encoder(gamma = 0, name=\"encoder\")\n",
    "encoder(Input(batch_shape=(None,128,128,1)))\n",
    "\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 93184)             1584128   \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 32, 32, 91)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 16)        93200     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 128, 128, 16)      16400     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 128, 128, 1)       65        \n",
      "=================================================================\n",
      "Total params: 1,693,793\n",
      "Trainable params: 1,693,793\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Build the decoder\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Decoder(Model):\n",
    "    def __init__(self, encoder_layer, Size_Kernals=8, Num_Kernals=16, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        Dense_Size = encoder_layer[1]\n",
    "        \n",
    "        DenseParam_Decode = 1500000\n",
    "        Dense_Depth = int(DenseParam_Decode / (latent_dim * Dense_Size * Dense_Size))\n",
    "        \n",
    "        self.dense1 = layers.Dense(Dense_Size * Dense_Size * Dense_Depth, activation=\"relu\",  kernel_regularizer = l2(0.1))\n",
    "        self.dense2 = layers.Reshape((Dense_Size, Dense_Size, Dense_Depth))\n",
    "                \n",
    "        self.convT1 = layers.Conv2DTranspose(Num_Kernals, kernel_size = (Size_Kernals, Size_Kernals), activation=\"relu\", strides=2, padding=\"same\")\n",
    "        self.convT2 = layers.Conv2DTranspose(Num_Kernals, kernel_size = (Size_Kernals, Size_Kernals), activation=\"relu\", strides=2, padding=\"same\")\n",
    "\n",
    "        self.outputs = layers.Conv2DTranspose(1, kernel_size = (2, 2), activation=\"relu\", padding= \"same\")\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "                \n",
    "        x = self.convT1(x)\n",
    "        x = self.convT2(x)\n",
    "        \n",
    "        output = self.outputs(x)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "decoder = Decoder(encoder.layers[1].output_shape, name=\"decoder\")\n",
    "decoder(Input(batch_shape=(None, latent_dim)))\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(Model):\n",
    "    def __init__(self, encoder, decoder, ZMCC_factor, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = metrics.Mean(name=\"kl_loss\")\n",
    "        self.ZMCC_factor = ZMCC_factor\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(x)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss= self.ZMCC_factor * tf.reduce_mean(ZMCC(reconstruction, y))\n",
    "            beta = 1\n",
    "            kl_loss = (-0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))) * beta\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result()\n",
    "        }\n",
    "\n",
    "    def call(self, data):\n",
    "        return self.decoder(self.encoder(data)[2])\n",
    "\n",
    "#losses.MSE(y, reconstruction), axis=(1, 2)\n",
    "#losses.mean_squared_logarithmic_error(y, reconstruction), axis=(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(batch_size=32, epochs=1500, patience=100, ZMCC_factor = 10000, kernal_size=8, kernal_num=16):\n",
    "    \n",
    "    encoder = Encoder(gamma = 0, name=\"encoder\", Size_Kernals=kernal_size, Num_Kernals=kernal_num)\n",
    "    encoder(Input(batch_shape=(None,128,128,1)))\n",
    "    \n",
    "    decoder = Decoder(encoder.layers[1].output_shape, name=\"decoder\", Size_Kernals=kernal_size, Num_Kernals=kernal_num)\n",
    "    decoder(Input(batch_shape=(None, latent_dim)))\n",
    "    \n",
    "    vae = VAE(encoder, decoder, ZMCC_factor)\n",
    "    vae.compile(optimizer=optimizers.Adam())\n",
    "    \n",
    "    \n",
    "\n",
    "    data_path = \"/home/ug-ml/felix-ML/VAE_000/DataAllInOne_Normalised/VAE_000_2/FilePaths/\"\n",
    "\n",
    "    TrainingPathsInput = gen_paths_fromfile(data_path + \"TrainingInput_0point1.txt\")\n",
    "    TrainingPathsOutput = gen_paths_fromfile(data_path + \"TrainingOutput_0point1.txt\")\n",
    "\n",
    "    ValidationPathsInput = gen_paths_fromfile(data_path + \"ValidationInput_0point1.txt\")\n",
    "    ValidationPathsOutput = gen_paths_fromfile(data_path + \"ValidationOutput_0point1.txt\")\n",
    "\n",
    "    TestPathsInput = gen_paths_fromfile(data_path + \"TestInput_0point1.txt\")\n",
    "    TestPathsOutput = gen_paths_fromfile(data_path + \"TestOutput_0point1.txt\")\n",
    "\n",
    "    train_seq = FelixSequence(TrainingPathsInput, TrainingPathsOutput, batch_size)\n",
    "    val_seq = FelixSequence(ValidationPathsInput, ValidationPathsOutput, batch_size)\n",
    "    test_seq = FelixSequence(TestPathsInput, TestPathsOutput, batch_size)\n",
    "\n",
    "    best_model_name = \"VAE_000_Normalised_0point1_zmcc10000\"\n",
    "    \n",
    "    best_model = vae\n",
    "\n",
    "\n",
    "    patience_i = 0\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    #training and validation histories, containing [0] the total loss, [1] the reconstruction loss, and [2] the kl loss.\n",
    "    #val_hist = np.zeros(shape=(1,epochs))\n",
    "    #train_hist = np.zeros(shape=(3,epochs))\n",
    "\n",
    "    for epoch in range(0, epochs):\n",
    "        print(\"-------------------------------------------------------------------------\")\n",
    "        print(\"Epoch\", epoch, \"/\", epochs, \": \")\n",
    "        print(\"Training: \")\n",
    "        vae.encoder.sampling.gamma=1\n",
    "        hist = vae.fit(x = train_seq, shuffle=True, epochs = epoch+1, workers = 16, initial_epoch=epoch)\n",
    "        #train_hist[0][epoch] = hist.history[\"loss\"][0]\n",
    "        #train_hist[1][epoch] = hist.history[\"reconstruction_loss\"][0]\n",
    "        #train_hist[2][epoch] = hist.history[\"kl_loss\"][0]\n",
    "        print(\"Validation: \")\n",
    "\n",
    "        tot_batch_recon_loss = 0\n",
    "        count = 0\n",
    "        vae.encoder.sampling.gamma=0\n",
    "        for x, y in val_seq:\n",
    "            count += 1\n",
    "            reconstruction = vae(x)\n",
    "            reconstruction_loss= ZMCC_factor * tf.reduce_mean(ZMCC(reconstruction, y))\n",
    "\n",
    "            tot_batch_recon_loss += reconstruction_loss\n",
    "\n",
    "        avg_recon_loss = float(tot_batch_recon_loss/count)\n",
    "        if(avg_recon_loss < best_val_loss):\n",
    "            #vae.save(\"/home/ug-ml/felix-ML/VAE_000/DataAllInOne_Normalised/VAE_000_2/Models/\"+str(best_model_name))\n",
    "            best_model = vae\n",
    "            print(\"The model improved from: \",best_val_loss, \"to: \", avg_recon_loss)\n",
    "            best_val_loss = avg_recon_loss\n",
    "            patience_i = 0\n",
    "        else:\n",
    "            patience_i+=1\n",
    "            print(\"The model did not improve, patience_i = \", patience_i)\n",
    "\n",
    "        print(\"Average reconstruction loss: \", avg_recon_loss)\n",
    "        #val_hist[0][epoch] = avg_recon_loss\n",
    "        if(patience_i > patience):\n",
    "            print(\"Early Stopping, the model did not improve from: \", best_val_loss)\n",
    "            break\n",
    "\n",
    "    print(\"-------------------------------------------------------------------------\")\n",
    "    \n",
    "    test_recon_loss = 0\n",
    "    count = 0\n",
    "    best_model.encoder.sampling.gamma=0\n",
    "    for x, y in test_seq:\n",
    "        count += 1\n",
    "        reconstruction = best_model(x)\n",
    "        reconstruction_loss= ZMCC_factor * tf.reduce_mean(ZMCC(reconstruction, y))\n",
    "\n",
    "        test_recon_loss += reconstruction_loss\n",
    "\n",
    "    avg_recon_loss = float(test_recon_loss/count)\n",
    "    \n",
    "    return best_model, avg_recon_loss\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATING OVER:\n",
      "[[5 3 3 6]\n",
      " [4 4 3 6]\n",
      " [4 3 4 6]\n",
      " [4 3 3 7]\n",
      " [3 3 3 6]\n",
      " [4 2 3 6]\n",
      " [4 3 2 6]\n",
      " [4 3 3 5]] 8\n",
      "Parameter set  1  of  8 +++++++++++++++++++++++++++++++++++++++++++++++++++.\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 0 / 1500 : \n",
      "Training: \n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [Op:Conv2D]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-cc1ed4945235>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mparameter_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_searched\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-cc1ed4945235>\u001b[0m in \u001b[0;36mparameter_search\u001b[0;34m(parameters, z, prev_searched)\u001b[0m\n\u001b[1;32m     64\u001b[0m                                              \u001b[0mZMCC_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                                              \u001b[0mkernal_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                                              kernal_num=param_set[2])\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0macc_norm_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-5c80dde44bda>\u001b[0m in \u001b[0;36mtrain_vae\u001b[0;34m(batch_size, epochs, patience, ZMCC_factor, kernal_size, kernal_num)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;31m#train_hist[0][epoch] = hist.history[\"loss\"][0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m#train_hist[1][epoch] = hist.history[\"reconstruction_loss\"][0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/felix-ML/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/felix-ML/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1061\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/felix-ML/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/felix-ML/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, shuffle, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[1;32m    914\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/felix-ML/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[1;32m    792\u001b[0m       \u001b[0mconcrete_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpack_x_y_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m       model.distribute_strategy.run(\n\u001b[0;32m--> 794\u001b[0;31m           lambda x: model(x, training=False), args=(concrete_x,))\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/felix-ML/env/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1209\u001b[0m       fn = autograph.tf_convert(\n\u001b[1;32m   1210\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m-> 1211\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m   \u001b[0;31m# TODO(b/151224785): Remove deprecated alias.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/felix-ML/env/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2583\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2584\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2585\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2587\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/felix-ML/env/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2943\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2944\u001b[0m         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):\n\u001b[0;32m-> 2945\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2947\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperimental_hints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/felix-ML/env/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/felix-ML/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    792\u001b[0m       \u001b[0mconcrete_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpack_x_y_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m       model.distribute_strategy.run(\n\u001b[0;32m--> 794\u001b[0;31m           lambda x: model(x, training=False), args=(concrete_x,))\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/felix-ML/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-1e0d69befaea>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m#losses.MSE(y, reconstruction), axis=(1, 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/felix-ML/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-d110253a4eec>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/felix-ML/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/felix-ML/env/lib/python3.6/site-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    245\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_causal_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/felix-ML/env/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/felix-ML/env/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_v2\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1015\u001b[0m       \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/felix-ML/env/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_internal\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution, num_spatial_dims)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m           \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m   1148\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mchannel_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/felix-ML/env/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_conv2d_expanded_batch\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   2589\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2590\u001b[0m         \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2591\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   2592\u001b[0m   return squeeze_batch_dims(\n\u001b[1;32m   2593\u001b[0m       \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/felix-ML/env/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    936\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    939\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/felix-ML/env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6842\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6843\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6844\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/felix-ML/env/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [Op:Conv2D]"
     ]
    }
   ],
   "source": [
    "parameters = {\"batch_size\": np.array([8, 12, 16, 24, 32, 48, 64, 86, 128]),\n",
    "             \"kernal_size\": np.array([2, 4, 6, 8, 10, 12, 14, 16, 18]),\n",
    "             \"kernal_num\": np.array([8, 16, 24, 32, 40, 48, 56, 64, 72]),\n",
    "             \"ZMCC_factor\": np.array([10, 50, 100, 500, 1000, 5000, 10000, 50000, 100000])}\n",
    "    \n",
    "z = np.array([4, 3, 3, 6])\n",
    "prev_searched = np.array([[4, 3, 3, 6]])\n",
    "\n",
    "epochs = 1500\n",
    "\n",
    "def neighbours(point, dircs):\n",
    "    ns = dircs+point\n",
    "    #print(dircs, point, prev_searched)\n",
    "    return np.array([i for i in ns if (0<=i).all() and (i<9).all() and not (i == prev_searched).all(axis=1).any()]).astype(int)\n",
    "\n",
    "def parameter_search(parameters, z, prev_searched):\n",
    "    \n",
    "    num_params = len(parameters)\n",
    "    dircs = np.zeros(shape = (2*num_params, num_params))\n",
    "    for i, dirc in enumerate(dircs):\n",
    "        if i < num_params:\n",
    "            dirc[i] = 1\n",
    "        else:\n",
    "            dirc[i-num_params] = -1\n",
    "    \n",
    "    best_params = {}\n",
    "    for key in parameters:\n",
    "        best_params[key] = np.nan\n",
    "\n",
    "    best_metrics = {\"test_loss\": np.inf, \"test_acc\": -np.inf}\n",
    "\n",
    "    best_hist = {\"train_acc\": np.zeros(shape=epochs),\n",
    "                \"train_loss\": np.zeros(shape=epochs),\n",
    "                \"val_acc\": np.zeros(shape=epochs),\n",
    "                \"val_loss\": np.zeros(shape=epochs)}\n",
    "    \n",
    "    converged = False\n",
    "        \n",
    "    while not converged:\n",
    "        print(\"ITERATING OVER:\")\n",
    "        neighs = neighbours(z, dircs)\n",
    "        num_neighs = neighs.shape[0]\n",
    "        print(neighs, num_neighs)\n",
    "\n",
    "        if neighs.size == 0:\n",
    "            print(\"No new neighbours available. Saving best model and parameter set so far.\")\n",
    "            #best_model.save(NewPath+ModelName)\n",
    "            converged = True\n",
    "            break\n",
    "\n",
    "        step_params = np.array([parameters[key][neighs[:,j]] for j, key in enumerate(parameters)]).T\n",
    "        #print(step_params)\n",
    "                \n",
    "\n",
    "        converged = True\n",
    "\n",
    "        for i, param_set in enumerate(step_params):\n",
    "            print(\"Parameter set \", i+1,\" of \", num_neighs,\"+++++++++++++++++++++++++++++++++++++++++++++++++++.\")\n",
    "            \n",
    "            acc_norm_factor = 10000/param_set[3]\n",
    "            \n",
    "            test_acc, best_model = train_vae(batch_size=param_set[0],\n",
    "                                             epochs=epochs,\n",
    "                                             ZMCC_factor=param_set[3], \n",
    "                                             kernal_size=param_set[1], \n",
    "                                             kernal_num=param_set[2])\n",
    "            \n",
    "            test_acc = test_acc*acc_norm_factor\n",
    "\n",
    "            prev_searched = np.append(prev_searched, neighs[i].reshape(1,num_params), axis=0)\n",
    "\n",
    "            if test_acc > best_metrics[\"test_acc\"]:\n",
    "                \n",
    "                best_metrics[\"test_acc\"] = test_acc\n",
    "                \n",
    "                for k, key in enumerate(best_params):\n",
    "                    best_params[key] = param_set[k]\n",
    "\n",
    "                z = neighs[i]\n",
    "                \n",
    "                #best_model.save(NewPath+ModelName)\n",
    "                converged = False\n",
    "        print(\"best params set:\" )\n",
    "        print(best_params)\n",
    "        print(\"best metrics set:\" )\n",
    "        print(best_metrics)\n",
    "                \n",
    "parameter_search(parameters, z, prev_searched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATING OVER:\n",
      "[[0 1 3]\n",
      " [0 2 2]\n",
      " [1 1 2]\n",
      " [0 1 1]\n",
      " [0 0 2]]\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 1 / 1 : \n",
      "Training: \n",
      "89/89 [==============================] - 7s 83ms/step - loss: 2.0421 - acc: 0.2734\n",
      "Validation: \n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.5310 - acc: 0.3549\n",
      "The model improved from:  inf to:  2.5309948921203613\n",
      "Epoch loss:  2.5309948921203613\n",
      "-------------------------------------------------------------------------\n",
      "Testing: \n",
      "6/6 [==============================] - 0s 20ms/step - loss: 1.9448 - acc: 0.3724\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 1 / 1 : \n",
      "Training: \n",
      "177/177 [==============================] - 7s 41ms/step - loss: 1.9997 - acc: 0.3010\n",
      "Validation: \n",
      "21/21 [==============================] - 1s 26ms/step - loss: 2.6480 - acc: 0.4045\n",
      "The model improved from:  inf to:  2.648041009902954\n",
      "Epoch loss:  2.648041009902954\n",
      "-------------------------------------------------------------------------\n",
      "Testing: \n",
      "11/11 [==============================] - 0s 14ms/step - loss: 1.8224 - acc: 0.4414\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 1 / 1 : \n",
      "Training: \n",
      "177/177 [==============================] - 6s 36ms/step - loss: 1.9428 - acc: 0.3107\n",
      "Validation: \n",
      "21/21 [==============================] - 0s 18ms/step - loss: 2.7648 - acc: 0.3955\n",
      "The model improved from:  inf to:  2.764773368835449\n",
      "Epoch loss:  2.764773368835449\n",
      "-------------------------------------------------------------------------\n",
      "Testing: \n",
      "11/11 [==============================] - 0s 18ms/step - loss: 2.0704 - acc: 0.4324\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 1 / 1 : \n",
      "Training: \n",
      "354/354 [==============================] - 10s 27ms/step - loss: 1.8249 - acc: 0.3443\n",
      "Validation: \n",
      "42/42 [==============================] - 1s 19ms/step - loss: 3.5550 - acc: 0.4195\n",
      "The model improved from:  inf to:  3.55496883392334\n",
      "Epoch loss:  3.55496883392334\n",
      "-------------------------------------------------------------------------\n",
      "Testing: \n",
      "21/21 [==============================] - 0s 16ms/step - loss: 2.4232 - acc: 0.4685\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 1 / 1 : \n",
      "Training: \n",
      "177/177 [==============================] - 6s 36ms/step - loss: 1.8983 - acc: 0.3507\n",
      "Validation: \n",
      "21/21 [==============================] - 1s 25ms/step - loss: 2.8816 - acc: 0.4060\n",
      "The model improved from:  inf to:  2.8816285133361816\n",
      "Epoch loss:  2.8816285133361816\n",
      "-------------------------------------------------------------------------\n",
      "Testing: \n",
      "11/11 [==============================] - 0s 15ms/step - loss: 2.0423 - acc: 0.4655\n",
      "ITERATING OVER:\n",
      "[[0 2 3]\n",
      " [0 3 2]\n",
      " [1 2 2]\n",
      " [0 2 1]]\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 1 / 1 : \n",
      "Training: \n",
      "89/89 [==============================] - 7s 78ms/step - loss: 2.1215 - acc: 0.2598\n",
      "Validation: \n",
      "11/11 [==============================] - 0s 25ms/step - loss: 2.8650 - acc: 0.3459\n",
      "The model improved from:  inf to:  2.8649563789367676\n",
      "Epoch loss:  2.8649563789367676\n",
      "-------------------------------------------------------------------------\n",
      "Testing: \n",
      "6/6 [==============================] - 0s 17ms/step - loss: 2.1361 - acc: 0.3724\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 1 / 1 : \n",
      "Training: \n",
      "177/177 [==============================] - 6s 34ms/step - loss: 2.1102 - acc: 0.2833\n",
      "Validation: \n",
      "21/21 [==============================] - 1s 25ms/step - loss: 3.0132 - acc: 0.3910\n",
      "The model improved from:  inf to:  3.0132031440734863\n",
      "Epoch loss:  3.0132031440734863\n",
      "-------------------------------------------------------------------------\n",
      "Testing: \n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.2549 - acc: 0.4084\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 1 / 1 : \n",
      "Training: \n",
      "177/177 [==============================] - 6s 36ms/step - loss: 2.1055 - acc: 0.3017\n",
      "Validation: \n",
      "21/21 [==============================] - 1s 25ms/step - loss: 2.3431 - acc: 0.3368\n",
      "The model improved from:  inf to:  2.3431174755096436\n",
      "Epoch loss:  2.3431174755096436\n",
      "-------------------------------------------------------------------------\n",
      "Testing: \n",
      "11/11 [==============================] - 0s 15ms/step - loss: 1.9427 - acc: 0.3453\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "-------------------------------------------------------------------------\n",
      "Epoch 1 / 1 : \n",
      "Training: \n",
      "354/354 [==============================] - 8s 23ms/step - loss: 1.9091 - acc: 0.3980\n",
      "Validation: \n",
      "42/42 [==============================] - 1s 17ms/step - loss: 2.3930 - acc: 0.4647\n",
      "The model improved from:  inf to:  2.393003225326538\n",
      "Epoch loss:  2.393003225326538\n",
      "-------------------------------------------------------------------------\n",
      "Testing: \n",
      "21/21 [==============================] - 0s 14ms/step - loss: 1.8335 - acc: 0.4985\n"
     ]
    }
   ],
   "source": [
    "prev_searched = np.array([[0,1,2]])\n",
    "\n",
    "learning_rate = np.array([0.0005, 0.0001, 0.001, 0.002, 0.005])\n",
    "l2_regularizer = np.array([0.00005, 0.0001, 0.0005, 0.001, 0.002])\n",
    "batch_size = np.array([8, 16, 32, 64, 128])\n",
    "\n",
    "def neighbours(point):\n",
    "    dircs = np.array([[0,0,1],[0,1,0],[1,0,0],[0,0,-1],[0,-1,0],[-1,0,0]])\n",
    "    ns = dircs+point\n",
    "    return np.array([i for i in ns if (0<=i).all() and (i<5).all() and not (i == prev_searched).all(axis=1).any()])\n",
    "\n",
    "\n",
    "z = np.array([0,1,2])\n",
    "converged = False\n",
    "best_test_loss = np.inf\n",
    "best_lr = np.nan\n",
    "best_l2_r = np.nan\n",
    "best_bs = np.nan\n",
    "\n",
    "while not converged:\n",
    "    print(\"ITERATING OVER:\")\n",
    "    neighs = neighbours(z)\n",
    "    print(neighs)\n",
    "    #print(neighs)\n",
    "    if neighs.size == 0:\n",
    "        print(\"No new neighbours available. Saving best model and parameter set so far.\")\n",
    "        np.save(NewPath+\"/parameter_search.npy\", np.array(best_lr,best_l2_r,best_bs))\n",
    "        best_model.save(NewPath+ModelName)\n",
    "        converged = True\n",
    "        break\n",
    "        \n",
    "\n",
    "    lr = learning_rate[neighs[:,0]]\n",
    "    l2_r = l2_regularizer[neighs[:,1]]\n",
    "    bs = batch_size[neighs[:,2]]\n",
    "\n",
    "    step_params = np.array([lr, l2_r, bs]).T\n",
    "    \n",
    "    converged = True\n",
    "    \n",
    "    for i, param_set in enumerate(step_params):\n",
    "        \n",
    "        model = build_model(param_set[0], param_set[1])\n",
    "        \n",
    "        test_loss, best_model = felix_fit_new(model, param_set[2].astype(int), epochs, CPUworkers, AllPaths, \"npy\", TrainingPatience)\n",
    "        \n",
    "        prev_searched = np.append(prev_searched, neighs[i].reshape(1,3), axis=0)\n",
    "        \n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            best_lr = param_set[0]\n",
    "            best_l2_r = param_set[1]\n",
    "            best_bs = param_set[2]\n",
    "            \n",
    "            z = neighs[i]\n",
    "            \n",
    "            np.save(NewPath+\"/parameter_search.npy\",param_set)\n",
    "            best_model.save(NewPath+ModelName)\n",
    "            converged = False\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005 0.0005 32.0\n",
      "1.822420358657837\n"
     ]
    }
   ],
   "source": [
    "print(best_lr,best_l2_r,best_bs)\n",
    "print(best_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 10ms/step - loss: 1.6834 - acc: 0.6216\n"
     ]
    }
   ],
   "source": [
    "test_seq = FelixSequence(AllPaths[2][0], AllPaths[2][1], best_bs.astype(int), \"npy\")\n",
    "tst_hist = best_model.evaluate(test_seq, workers = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build model\n",
    "strategy = MirroredStrategy() #Allows multiple GPUs\n",
    "\n",
    "with strategy.scope():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(128, (4, 4),\n",
    "                                     activation='relu',\n",
    "                                     data_format='channels_first',\n",
    "                                     input_shape= input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2), data_format='channels_first'))\n",
    "    model.add(layers.Conv2D(128, (4, 4),\n",
    "                                     data_format='channels_first',\n",
    "                                     activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2), data_format='channels_first'))\n",
    "    model.add(layers.Conv2D(128, (4, 4),\n",
    "                                     data_format='channels_first',\n",
    "                                     activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2), data_format='channels_first'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    model.add(layers.Dense(128, activation='relu',\n",
    "                           kernel_regularizer = l2(l2_regularizer)))\n",
    "    \n",
    "    model.add(layers.Dense(10, activation='softmax',\n",
    "                           kernel_regularizer = l2(l2_regularizer)))\n",
    "\n",
    "    model.compile(loss = loss,\n",
    "                  optimizer = optimizers.RMSprop(learning_rate = learning_rate),\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "#Make folder to put model and history information\n",
    "try:\n",
    "    os.mkdir(NewPath)\n",
    "except:\n",
    "    print(\"Folder failed to be created, it may already exist\")\n",
    "    \n",
    "File1  = open(NewPath +\"/Parameters.txt\", \"w+\")\n",
    "if(len(VariableListName) == len(VariableListValues)):\n",
    "    for i in range(0, len(VariableListName)):\n",
    "        File1.write(VariableListName[i] + \" \" + str(VariableListValues[i]) + \"\\n\")\n",
    "    File1.close()\n",
    "else:\n",
    "    print(\"VariableListName and VariableListValues do not match up, so file can not be saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history = np.zeros(shape=(2,epochs))\n",
    "validation_history = np.zeros(shape=(2,epochs))\n",
    "test_history = [0,0]\n",
    "#print(model.metrics_names)\n",
    "felix_fit_new(model, batch_size, epochs, CPUworkers, AllPaths, \"npy\",TrainingPatience)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
